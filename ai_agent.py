# -----------------------------------------------------------
# Environment configuration
# Load API keys from environment variables
# -----------------------------------------------------------
# if you dont use pipenv uncomment the following:
# from dotenv import load_dotenv
# load_dotenv()

#Step1: Setup API Keys for Groq, OpenAI and Tavily
import os
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_core.messages.ai import AIMessage
from langgraph.prebuilt import create_react_agent
from langchain_community.tools.tavily_search import TavilySearchResults


# -----------------------------------------------------------
# API keys for external services
# -----------------------------------------------------------
GROQ_API_KEY=os.environ.get("GROQ_API_KEY")
TAVILY_API_KEY=os.environ.get("TAVILY_API_KEY")
#OPENAI_API_KEY=os.environ.get("OPENAI_API_KEY")



# Step2: Setup LLM & Tools
# -----------------------------------------------------------
# Default LLM configuration (optional / reference)
# -----------------------------------------------------------
# openai_llm=ChatOpenAI(model="gpt-4o-mini")
groq_llm=ChatGroq(model="llama-3.3-70b-versatile")



# -----------------------------------------------------------
# Search tool configuration
# -----------------------------------------------------------
search_tool=TavilySearchResults(max_results=2)


#Step3: Setup AI Agent with Search tool functionality
# -----------------------------------------------------------
# Default system prompt (fallback behavior)
# -----------------------------------------------------------
system_prompt="Act as an AI chatbot who is smart and friendly"



# -----------------------------------------------------------
# Core AI agent interaction function
# -----------------------------------------------------------
def get_response_from_ai_agent(llm_id, query, allow_search, system_prompt, provider):
    """
    Generate a response from an AI agent using LangGraph and optional search tools.

    This function dynamically selects the LLM provider and model,
    constructs a ReAct-style agent, and executes it with the provided
    user messages and system instructions.

    Args:
        llm_id (str):
            Identifier of the language model to use (e.g., "llama-3.3-70b-versatile").

        query (list[str]):
            A list of user messages representing the conversation context.
            Only user messages are passed to the agent.

        allow_search (bool):
            Determines whether external web search tools (Tavily)
            are available to the agent during response generation.

        system_prompt (str):
            High-level instruction that defines the agentâ€™s behavior,
            tone, and response style.

        provider (str):
            LLM provider name (e.g., "Groq", "OpenAI").
            Used to initialize the appropriate model backend.

    Returns:
        str:
            The final response generated by the AI agent.

    Raises:
        ValueError:
            If an unsupported provider is specified.
    """

    # -------------------------------------------------------
    # Initialize LLM based on selected provider
    # -------------------------------------------------------
    if provider=="Groq":
        llm=ChatGroq(model=llm_id)
    # elif provider=="OpenAI":
    #     llm=ChatOpenAI(model=llm_id)


    # -------------------------------------------------------
    # Configure tools based on user preference
    # -------------------------------------------------------
    tools=[TavilySearchResults(max_results=2)] if allow_search else []
    

    # -------------------------------------------------------
    # Create a ReAct-based AI agent
    # -------------------------------------------------------
    agent=create_react_agent(
        model=llm,
        tools=tools,
        state_modifier=system_prompt
    )


    # -------------------------------------------------------
    # Prepare agent input state
    # -------------------------------------------------------
    state={"messages": query}


    # -------------------------------------------------------
    # Execute agent and collect response
    # -------------------------------------------------------
    response=agent.invoke(state)

     # -------------------------------------------------------
    # Extract the final AI-generated message
    # -------------------------------------------------------
    messages=response.get("messages")
    ai_messages=[message.content for message in messages if isinstance(message, AIMessage)]
    return ai_messages[-1]

